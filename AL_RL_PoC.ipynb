{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martiny76/Files-PdfConverter/blob/main/AL_RL_PoC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ENV = 'COLAB'  #@param [\"COLAB\", \"CONDA\"]\n",
        "SAMPLING_STRATEGY = \"random\"  #@param [\"uncertainty\", \"margin\", \"random\"]\n",
        "DATASET_NAME = \"SetFit/subj\" #@param [\"ag_news\", \"SetFit/subj\"]\n",
        "MODEL_NAME = 'bert-base-uncased' #@param ['bert-base-uncased']\n",
        "NUM_CLASSES = {\n",
        "    \"ag_news\": 4,\n",
        "    \"SetFit/subj\": 2\n",
        "}\n",
        "BATCH_SIZE_TRAIN = 1\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "L_4IaqgLB1gk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ENV == 'COLAB':\n",
        "  !pip install datasets --quiet"
      ],
      "metadata": {
        "id": "zyNfuVTWBtwj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data part"
      ],
      "metadata": {
        "id": "jXMQy2bAJAMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jeV4C9OIBhNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dec6078-2668-44b9-c2da-0511576b8571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Process dataset\n",
        "class AGNewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def encode_data(tokenizer, texts, labels):\n",
        "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
        "    return AGNewsDataset(encodings, labels)\n",
        "\n",
        "# Test set is pre-defined\n",
        "test_texts, test_labels = dataset['train']['text'], dataset['train']['label']\n",
        "\n",
        "# Split train data into training and dev sets\n",
        "train_texts, dev_texts, train_labels, dev_labels = train_test_split(\n",
        "    dataset['train']['text'], dataset['train']['label'], test_size=0.2\n",
        ")\n",
        "\n",
        "# Encode the data\n",
        "train_dataset = encode_data(tokenizer, train_texts, train_labels)\n",
        "dev_dataset = encode_data(tokenizer, dev_texts, dev_labels)\n",
        "test_dataset = encode_data(tokenizer, test_texts, test_labels)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT part"
      ],
      "metadata": {
        "id": "CmPV710YJDQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = bert_model  # Pre-loaded BERT model\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, 512),  # BERT base outputs 768 features from CLS token\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get the outputs from BERT model\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # We use the output associated with the CLS token\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, 768)\n",
        "\n",
        "        # Pass CLS token embeddings through the classifier to get final logits\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "zSH5UOGqGR__"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "# Load pre-trained BERT\n",
        "bert_model = BertModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Initialize the classifier\n",
        "num_classes = NUM_CLASSES[DATASET_NAME]  # Assuming 4 classes for AG News dataset\n",
        "model = BertClassifier(bert_model, num_classes)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBM-rsU4JPJF",
        "outputId": "4376aaa6-67bb-4fc1-8ddf-44f005aff1a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.1, inplace=False)\n",
              "    (3): Linear(in_features=512, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a given dataset.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The model to be evaluated.\n",
        "        data_loader (DataLoader): DataLoader for the dataset to evaluate on.\n",
        "        device (torch.device): Device to perform computation (GPU/CPU).\n",
        "\n",
        "    Returns:\n",
        "        float: Accuracy of the model on the provided dataset.\n",
        "        float: Precision, Recall, and F1-Score of the model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, predictions = torch.max(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def evaluate_on_datasets(model, dev_loader, test_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on development and test datasets.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The model to be evaluated.\n",
        "        dev_loader (DataLoader): DataLoader for the development dataset.\n",
        "        test_loader (DataLoader): DataLoader for the test dataset.\n",
        "        device (torch.device): Device to perform computation (GPU/CPU).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing accuracy, precision, recall, and F1-score for both datasets.\n",
        "    \"\"\"\n",
        "    print(\"Evaluating on development dataset:\")\n",
        "    dev_accuracy, dev_precision, dev_recall, dev_f1 = evaluate_model(model, dev_loader, device)\n",
        "    print(f\"Development Set - Accuracy: {dev_accuracy:.4f}, Precision: {dev_precision:.4f}, Recall: {dev_recall:.4f}, F1-Score: {dev_f1:.4f}\")\n",
        "\n",
        "    print(\"Evaluating on test dataset:\")\n",
        "    test_accuracy, test_precision, test_recall, test_f1 = evaluate_model(model, test_loader, device)\n",
        "    print(f\"Test Set - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'dev': {\n",
        "            'accuracy': dev_accuracy,\n",
        "            'precision': dev_precision,\n",
        "            'recall': dev_recall,\n",
        "            'f1': dev_f1\n",
        "        },\n",
        "        'test': {\n",
        "            'accuracy': test_accuracy,\n",
        "            'precision': test_precision,\n",
        "            'recall': test_recall,\n",
        "            'f1': test_f1\n",
        "        }\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Y4HEsBy2zRFa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "def train_model(model, data_loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    best_performance = {\n",
        "        'dev': {'accuracy': 0},\n",
        "        'test': {'accuracy': 0}\n",
        "    }\n",
        "\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Zero the gradients on each iteration\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute loss between predictions and labels\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    #\n",
        "    eval_results = evaluate_on_datasets(model, dev_loader, test_loader, device)\n",
        "    print(12 * ' = ')\n",
        "\n",
        "    if eval_results['dev']['accuracy'] > best_performance['dev']['accuracy']:\n",
        "        best_performance['dev'] = eval_results['dev']\n",
        "        best_performance['test'] = eval_results['test']\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Average loss:\", total_loss / len(data_loader))\n",
        "    print(best_performance)\n",
        "    return best_performance\n",
        "\n"
      ],
      "metadata": {
        "id": "_x4IkDmpN8Tq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example training call\n",
        "# train_model(model, train_loader, optimizer)\n"
      ],
      "metadata": {
        "id": "XSb5J9hY4TcV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Active Learning part"
      ],
      "metadata": {
        "id": "RoLKtvCKK2bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def sample_data(logits, k, used_indices, total_samples, method='uncertainty'):\n",
        "    \"\"\"\n",
        "    Selects indices of k most uncertain samples based on the specified sampling method,\n",
        "    excluding already used indices.\n",
        "\n",
        "    Parameters:\n",
        "        logits (torch.Tensor): Logits from the model for the unlabeled data.\n",
        "        k (int): Number of samples to query.\n",
        "        used_indices (set): Set of indices that have already been used.\n",
        "        total_samples (int): Total number of samples in the dataset.\n",
        "        method (str): Sampling method ('uncertainty', 'random', 'margin').\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Indices of the selected samples.\n",
        "    \"\"\"\n",
        "    valid_indices = torch.tensor([i for i in range(total_samples) if i not in used_indices], dtype=torch.long)\n",
        "    logits = logits[valid_indices]\n",
        "\n",
        "    if method == 'random':\n",
        "        indices = torch.randperm(logits.size(0))[:k]\n",
        "    elif method == 'margin':\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        top2_probs = torch.topk(probs, 2, dim=1).values\n",
        "        margins = top2_probs[:, 0] - top2_probs[:, 1]\n",
        "        indices = torch.topk(margins, k, largest=False).indices\n",
        "    else:  # Default to uncertainty sampling\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        log_probs = torch.log(probs + 1e-5)  # Adding a small constant to avoid log(0)\n",
        "        entropy = -(probs * log_probs).sum(dim=1)\n",
        "        indices = torch.topk(entropy, k).indices\n",
        "\n",
        "    return valid_indices[indices]\n",
        "\n",
        "\n",
        "def select_action(model, data_loader, dqn, device, query_budget, used_indices, sampling_method='uncertainty'):\n",
        "    \"\"\"\n",
        "    Select actions for each sample in the dataset using the DQN, considering a query budget.\n",
        "    \"\"\"\n",
        "    selected_indices = []\n",
        "    all_rewards = []\n",
        "    model.eval()\n",
        "    dqn.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        all_logits = []\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            all_logits.append(outputs.logits)\n",
        "        all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "        query_indices = sample_data(all_logits, min(query_budget, len(all_logits) - len(used_indices)), used_indices, len(all_logits), method=sampling_method)\n",
        "\n",
        "        # Update used_indices with new selections\n",
        "        used_indices.update(query_indices.tolist())\n",
        "\n",
        "        # Process selected samples\n",
        "        for idx in query_indices:\n",
        "            if query_budget <= 0:\n",
        "                break\n",
        "            selected_indices.append(idx)\n",
        "            cls_embeddings = all_logits[idx].unsqueeze(0)  # Extract the CLS token embeddings for this index\n",
        "            action_probs = dqn(cls_embeddings)\n",
        "            action = torch.argmax(action_probs, dim=1).item()\n",
        "            if action == 1:\n",
        "                reward = calculate_reward(cls_embeddings, model)\n",
        "                all_rewards.append(reward)\n",
        "                query_budget -= 1\n",
        "\n",
        "    return selected_indices, all_rewards, query_budget, used_indices\n",
        "\n"
      ],
      "metadata": {
        "id": "K2yLbMpxf0fg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def uncertainty_sampling(logits, k):\n",
        "    \"\"\"Selects the indices of k most uncertain samples based on entropy of predictions.\"\"\"\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    log_probs = torch.log(probs + 1e-5)  # Adding a small constant to avoid log(0)\n",
        "    entropy = -(probs * log_probs).sum(dim=1)\n",
        "    return torch.topk(entropy, k).indices\n"
      ],
      "metadata": {
        "id": "cFzYEfLWJ96D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3AVxYLsfT0p"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not False:\n",
        "  from torch.utils.data import DataLoader, Subset, random_split\n",
        "  from transformers import BertTokenizer, BertModel\n",
        "\n",
        "  # Assume data_loader for the entire dataset and model are already defined\n",
        "  num_iterations = 10\n",
        "  k = 10  # Number of samples to query each iteration\n",
        "\n",
        "  # Split data into initial labeled and unlabeled sets\n",
        "  initial_labeled_size = 100\n",
        "  labeled_dataset, unlabeled_dataset = random_split(train_dataset, [initial_labeled_size, len(train_dataset) - initial_labeled_size])\n",
        "\n",
        "  labeled_loader = DataLoader(labeled_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
        "  unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=False)\n",
        "\n",
        "  for iteration in range(num_iterations):\n",
        "      # Train model on the current labeled dataset\n",
        "      train_model(model, labeled_loader, optimizer)\n",
        "\n",
        "      # Make predictions on the unlabeled dataset to find the most uncertain samples\n",
        "      model.eval()\n",
        "      all_logits = []\n",
        "      with torch.no_grad():\n",
        "          for batch in unlabeled_loader:\n",
        "              input_ids = batch['input_ids'].to(device)\n",
        "              attention_mask = batch['attention_mask'].to(device)\n",
        "              logits = model(input_ids, attention_mask)\n",
        "              all_logits.append(logits)\n",
        "      all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "      # Initialization of used indices\n",
        "      used_indices = set()\n",
        "\n",
        "      # Uncertainty sampling\n",
        "      query_indices = sample_data(all_logits, k, used_indices, len(all_logits), method=SAMPLING_STRATEGY)\n",
        "\n",
        "      # Update labeled and unlabeled sets: simulate the labeling process\n",
        "      newly_labeled = Subset(unlabeled_dataset, query_indices)\n",
        "      labeled_dataset = torch.utils.data.ConcatDataset([labeled_dataset, newly_labeled])\n",
        "      unlabeled_dataset = Subset(unlabeled_dataset, [i for i in range(len(unlabeled_dataset)) if i not in query_indices])\n",
        "\n",
        "      # Update DataLoaders\n",
        "      labeled_loader = DataLoader(labeled_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
        "      unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=False)\n",
        "\n",
        "      print(f\"Iteration {iteration + 1}: Model trained on an additional {k} samples\")\n",
        "      print(f\"Using {( len(labeled_loader) / (len(labeled_loader) + len(unlabeled_loader)) ) * 100} % of data.\")\n"
      ],
      "metadata": {
        "id": "9b9vEOERK6Xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21eeec7-720d-4602-a5cf-d10067715df6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9094, Precision: 0.9099, Recall: 0.9108, F1-Score: 0.9094\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9129, Precision: 0.9137, Recall: 0.9131, F1-Score: 0.9129\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.42332098495215176\n",
            "{'dev': {'accuracy': 0.909375, 'precision': 0.9099271026751943, 'recall': 0.9107778016191606, 'f1': 0.9093528693528694}, 'test': {'accuracy': 0.912875, 'precision': 0.9137206678254006, 'recall': 0.9130701770889281, 'f1': 0.9128523402467819}}\n",
            "Iteration 1: Model trained on an additional 10 samples\n",
            "Using 1.7187500000000002 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9175, Precision: 0.9268, Recall: 0.9141, F1-Score: 0.9164\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9119, Precision: 0.9207, Recall: 0.9113, F1-Score: 0.9113\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.1324831052937291\n",
            "{'dev': {'accuracy': 0.9175, 'precision': 0.9267792320567637, 'recall': 0.9141271678473373, 'f1': 0.9164185341900435}, 'test': {'accuracy': 0.911875, 'precision': 0.9207225063938619, 'recall': 0.9112842430337915, 'f1': 0.9113197940147497}}\n",
            "Iteration 2: Model trained on an additional 10 samples\n",
            "Using 1.875 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9175, Precision: 0.9205, Recall: 0.9156, F1-Score: 0.9169\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9150, Precision: 0.9191, Recall: 0.9146, F1-Score: 0.9147\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.1399151214864105\n",
            "{'dev': {'accuracy': 0.9175, 'precision': 0.920535678909624, 'recall': 0.9155931518499434, 'f1': 0.9169172524157725}, 'test': {'accuracy': 0.915, 'precision': 0.919083067582146, 'recall': 0.9146002499795143, 'f1': 0.9147326014381099}}\n",
            "Iteration 3: Model trained on an additional 10 samples\n",
            "Using 2.03125 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9156, Precision: 0.9234, Recall: 0.9125, F1-Score: 0.9146\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9071, Precision: 0.9159, Recall: 0.9065, F1-Score: 0.9065\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.0145648814093035\n",
            "{'dev': {'accuracy': 0.915625, 'precision': 0.9234218158201027, 'recall': 0.9125225731778179, 'f1': 0.9146269017746933}, 'test': {'accuracy': 0.907125, 'precision': 0.9158631713554988, 'recall': 0.9065339197149106, 'f1': 0.9065398680183816}}\n",
            "Iteration 4: Model trained on an additional 10 samples\n",
            "Using 2.1875 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9056, Precision: 0.9126, Recall: 0.9026, F1-Score: 0.9046\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9044, Precision: 0.9117, Recall: 0.9038, F1-Score: 0.9039\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.11252167307039988\n",
            "{'dev': {'accuracy': 0.905625, 'precision': 0.9125581768576472, 'recall': 0.9026209162713261, 'f1': 0.9045602125905463}, 'test': {'accuracy': 0.904375, 'precision': 0.9117010397701306, 'recall': 0.9038311732592299, 'f1': 0.9038573786422088}}\n",
            "Iteration 5: Model trained on an additional 10 samples\n",
            "Using 2.34375 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9181, Precision: 0.9207, Recall: 0.9164, F1-Score: 0.9176\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9140, Precision: 0.9166, Recall: 0.9137, F1-Score: 0.9138\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.044490600681553284\n",
            "{'dev': {'accuracy': 0.918125, 'precision': 0.920707390409875, 'recall': 0.9163723474068841, 'f1': 0.9175892982469247}, 'test': {'accuracy': 0.914, 'precision': 0.9165764243262509, 'recall': 0.9136826875279198, 'f1': 0.9138176381222667}}\n",
            "Iteration 6: Model trained on an additional 10 samples\n",
            "Using 2.5 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.8688, Precision: 0.8788, Recall: 0.8729, F1-Score: 0.8685\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.8812, Precision: 0.8927, Recall: 0.8820, F1-Score: 0.8805\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.08962265146401478\n",
            "{'dev': {'accuracy': 0.86875, 'precision': 0.8787862878270816, 'recall': 0.872896367774038, 'f1': 0.8685125007043972}, 'test': {'accuracy': 0.88125, 'precision': 0.8927230229034144, 'recall': 0.8819597158831622, 'f1': 0.8805089689024591}}\n",
            "Iteration 7: Model trained on an additional 10 samples\n",
            "Using 2.65625 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9213, Precision: 0.9282, Recall: 0.9184, F1-Score: 0.9204\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9207, Precision: 0.9260, Recall: 0.9203, F1-Score: 0.9204\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.022101954787777846\n",
            "{'dev': {'accuracy': 0.92125, 'precision': 0.9281710481814349, 'recall': 0.9183747625215551, 'f1': 0.9203931064220966}, 'test': {'accuracy': 0.92075, 'precision': 0.9259798357126174, 'recall': 0.9203011379962049, 'f1': 0.9204441874565832}}\n",
            "Iteration 8: Model trained on an additional 10 samples\n",
            "Using 2.8125 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.8838, Precision: 0.8902, Recall: 0.8871, F1-Score: 0.8837\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.8931, Precision: 0.8998, Recall: 0.8937, F1-Score: 0.8928\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.057692698870475095\n",
            "{'dev': {'accuracy': 0.88375, 'precision': 0.8902183162072432, 'recall': 0.8871380264660231, 'f1': 0.8836698412499863}, 'test': {'accuracy': 0.893125, 'precision': 0.8997835722062433, 'recall': 0.893663293707928, 'f1': 0.8927658174299475}}\n",
            "Iteration 9: Model trained on an additional 10 samples\n",
            "Using 2.96875 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9225, Precision: 0.9337, Recall: 0.9188, F1-Score: 0.9214\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9123, Precision: 0.9217, Recall: 0.9116, F1-Score: 0.9117\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.020281820018212066\n",
            "{'dev': {'accuracy': 0.9225, 'precision': 0.933701594736219, 'recall': 0.9188336656334821, 'f1': 0.9213676948051948}, 'test': {'accuracy': 0.91225, 'precision': 0.9217105971317243, 'recall': 0.9116396734752759, 'f1': 0.9116632451903657}}\n",
            "Iteration 10: Model trained on an additional 10 samples\n",
            "Using 3.125 % of data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmFA-_nrRowV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Active Learning + Reinforcement Learning"
      ],
      "metadata": {
        "id": "hor3aypJSp33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 2)  # Binary output for each sample\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "KLYkK07AgWC9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(model, data_loader, dqn, device, query_budget=5):\n",
        "    \"\"\"\n",
        "    Select actions for each sample in the dataset using the DQN, considering a query budget.\n",
        "\n",
        "    Parameters:\n",
        "        model (nn.Module): The BERT-based classification model.\n",
        "        data_loader (DataLoader): DataLoader for the unlabeled dataset.\n",
        "        dqn (DQN): The trained DQN model to decide on actions.\n",
        "        device (torch.device): Device to perform computation (GPU/CPU).\n",
        "        query_budget (int): Remaining budget for querying labels.\n",
        "\n",
        "    Returns:\n",
        "        list: Indices of samples selected for labeling.\n",
        "        list: Rewards accumulated for each selected sample.\n",
        "        int: Updated query budget after selecting actions.\n",
        "    \"\"\"\n",
        "    selected_indices = []\n",
        "    all_rewards = []\n",
        "    model.eval()\n",
        "    dqn.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(data_loader):\n",
        "            if query_budget <= 0:  # Stop querying if the budget is exhausted\n",
        "                break\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Extract the CLS token embeddings\n",
        "\n",
        "            # DQN takes the state (CLS embeddings) and outputs action probabilities\n",
        "            action_probs = dqn(cls_embeddings)\n",
        "            action = torch.argmax(action_probs, dim=1).item()  # Choosing the action with the highest probability\n",
        "\n",
        "            # Append the index of the sample if the action is to label it\n",
        "            if action == 1:  # Action 1 implies query the label\n",
        "                if query_budget > 0:\n",
        "                    selected_indices.append(idx)\n",
        "                    reward = calculate_reward(cls_embeddings, model)\n",
        "                    all_rewards.append(reward)\n",
        "                    query_budget -= 1  # Decrement the budget\n",
        "\n",
        "    return selected_indices, all_rewards\n",
        "\n",
        "def calculate_reward(cls_embeddings, model):\n",
        "    \"\"\"\n",
        "    Calculate the reward for labeling a sample based on model performance improvement and query cost.\n",
        "\n",
        "    Parameters:\n",
        "        cls_embeddings (torch.Tensor): CLS token embeddings from BERT model.\n",
        "        model (nn.Module): The BERT-based classification model.\n",
        "\n",
        "    Returns:\n",
        "        float: Computed reward for the action taken.\n",
        "    \"\"\"\n",
        "    performance_improvement = simulate_model_improvement(cls_embeddings, model)\n",
        "    query_cost = 0.1  # Assuming a fixed cost for querying\n",
        "    return performance_improvement - query_cost\n",
        "\n",
        "def simulate_model_improvement(cls_embeddings, model):\n",
        "    \"\"\"\n",
        "    Simulate an improvement in model performance to calculate the reward.\n",
        "    This is a placeholder function and should be implemented based on actual model evaluations.\n",
        "\n",
        "    Returns:\n",
        "        float: Simulated improvement value.\n",
        "    \"\"\"\n",
        "    return 0.05  # Example fixed improvement for demonstration\n"
      ],
      "metadata": {
        "id": "3q3JW3eXhrN0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "batch_size = 1\n",
        "# Assuming data_loader for the entire dataset and model are already defined\n",
        "num_iterations = 10\n",
        "k = 10  # Number of samples to query each iteration\n",
        "\n",
        "# Split data into initial labeled and unlabeled sets\n",
        "initial_labeled_size = 100\n",
        "labeled_dataset, unlabeled_dataset = random_split(train_dataset, [initial_labeled_size, len(train_dataset) - initial_labeled_size])\n",
        "\n",
        "labeled_loader = DataLoader(labeled_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=False)\n",
        "\n",
        "# Parameters\n",
        "input_dim = 768  # Assuming using BERT's CLS token embedding as state\n",
        "hidden_dim = 256\n",
        "\n",
        "# Initialize DQN\n",
        "dqn = DQN(input_dim, hidden_dim).to(device)\n",
        "optimizer_dqn = optim.Adam(dqn.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Experience replay buffer\n",
        "replay_buffer = deque(maxlen=10000)\n",
        "\n",
        "# Active learning loop\n",
        "for iteration in range(num_iterations):\n",
        "    # Train model on the current labeled dataset\n",
        "    train_model(model, labeled_loader, optimizer)\n",
        "\n",
        "    # Evaluate model on the unlabeled dataset and select actions\n",
        "    new_indices, rewards = select_action(bert_model, unlabeled_loader, dqn, device, query_budget=k)\n",
        "    print(f\"Iteration {iteration + 1}: Model trained on newly labeled samples. Total rewards: {sum(rewards)}\")\n",
        "    print(f\"Using {( len(labeled_loader) / (len(labeled_loader) + len(unlabeled_loader)) ) * 100} % of data.\")\n",
        "\n",
        "    # Update labeled and unlabeled sets based on the actions taken\n",
        "    if new_indices:\n",
        "        newly_labeled = Subset(unlabeled_dataset, new_indices)\n",
        "        remaining_indices = [i for i in range(len(unlabeled_dataset)) if i not in new_indices]\n",
        "        unlabeled_dataset = Subset(unlabeled_dataset, remaining_indices)\n",
        "        labeled_dataset = torch.utils.data.ConcatDataset([labeled_dataset, newly_labeled])\n",
        "\n",
        "        # Update DataLoaders for the next iteration\n",
        "        labeled_loader = DataLoader(labeled_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
        "        unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=False)\n",
        "\n",
        "    # Optional: Update DQN based on the collected experience\n",
        "    if len(replay_buffer) >= batch_size:\n",
        "        train_dqn(dqn, replay_buffer, optimizer_dqn, batch_size)\n"
      ],
      "metadata": {
        "id": "VZFb1LPrhuR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e43b8a-2bbe-4df2-b750-e12318354d9b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.8988, Precision: 0.9018, Recall: 0.9012, F1-Score: 0.8987\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9051, Precision: 0.9088, Recall: 0.9055, F1-Score: 0.9050\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.2772319319087546\n",
            "{'dev': {'accuracy': 0.89875, 'precision': 0.9017501595830608, 'recall': 0.9011964371576826, 'f1': 0.8987460447673736}, 'test': {'accuracy': 0.905125, 'precision': 0.9088075914910777, 'recall': 0.9055238509671064, 'f1': 0.9049662208285465}}\n",
            "Iteration 1: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 1.5625 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9287, Precision: 0.9347, Recall: 0.9261, F1-Score: 0.9280\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9280, Precision: 0.9329, Recall: 0.9276, F1-Score: 0.9277\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.020294637626714327\n",
            "{'dev': {'accuracy': 0.92875, 'precision': 0.9347436001911035, 'recall': 0.9261369598686879, 'f1': 0.9280483590765574}, 'test': {'accuracy': 0.928, 'precision': 0.9329456840110351, 'recall': 0.9275681326060254, 'f1': 0.9277398635086311}}\n",
            "Iteration 2: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 1.7187500000000002 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9381, Precision: 0.9406, Recall: 0.9365, F1-Score: 0.9377\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9374, Precision: 0.9390, Recall: 0.9371, F1-Score: 0.9373\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.0037697059247875585\n",
            "{'dev': {'accuracy': 0.938125, 'precision': 0.9405963288249837, 'recall': 0.9365421572205195, 'f1': 0.9377450257921887}, 'test': {'accuracy': 0.937375, 'precision': 0.9390150997746183, 'recall': 0.9371325023309399, 'f1': 0.9372885669875846}}\n",
            "Iteration 3: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 1.875 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.8881, Precision: 0.9095, Recall: 0.8827, F1-Score: 0.8854\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.8840, Precision: 0.9044, Recall: 0.8831, F1-Score: 0.8823\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.034632279358965415\n",
            "{'dev': {'accuracy': 0.888125, 'precision': 0.9094675032175032, 'recall': 0.882697003347017, 'f1': 0.8853937449154479}, 'test': {'accuracy': 0.884, 'precision': 0.9043984803079608, 'recall': 0.8830783545205045, 'f1': 0.8823337279210905}}\n",
            "Iteration 4: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 2.03125 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.8694, Precision: 0.8809, Recall: 0.8738, F1-Score: 0.8691\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.8758, Precision: 0.8880, Recall: 0.8765, F1-Score: 0.8749\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.1428411017167881\n",
            "{'dev': {'accuracy': 0.869375, 'precision': 0.8809181797287979, 'recall': 0.8737977286645293, 'f1': 0.8690717681693265}, 'test': {'accuracy': 0.87575, 'precision': 0.8879716750983992, 'recall': 0.8764861558389818, 'f1': 0.8749088873585991}}\n",
            "Iteration 5: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 2.1875 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.8225, Precision: 0.8513, Recall: 0.8295, F1-Score: 0.8206\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.8409, Precision: 0.8666, Recall: 0.8420, F1-Score: 0.8383\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.07066868217041095\n",
            "{'dev': {'accuracy': 0.8225, 'precision': 0.8513053803413879, 'recall': 0.8294681579190546, 'f1': 0.8206153399149818}, 'test': {'accuracy': 0.840875, 'precision': 0.8666347105683503, 'recall': 0.8419687127455112, 'f1': 0.8383234792520555}}\n",
            "Iteration 6: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 2.34375 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9319, Precision: 0.9327, Recall: 0.9309, F1-Score: 0.9316\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9323, Precision: 0.9331, Recall: 0.9321, F1-Score: 0.9322\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.0257681823502935\n",
            "{'dev': {'accuracy': 0.931875, 'precision': 0.9326942431561998, 'recall': 0.9309491776550207, 'f1': 0.9315915055791756}, 'test': {'accuracy': 0.93225, 'precision': 0.9330591575349387, 'recall': 0.9320803772206746, 'f1': 0.9321958880809316}}\n",
            "Iteration 7: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 2.5 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9081, Precision: 0.9169, Recall: 0.9048, F1-Score: 0.9069\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9077, Precision: 0.9162, Recall: 0.9072, F1-Score: 0.9072\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.06579028968938955\n",
            "{'dev': {'accuracy': 0.908125, 'precision': 0.9169344794429495, 'recall': 0.9047603758306851, 'f1': 0.9069340098027927}, 'test': {'accuracy': 0.90775, 'precision': 0.9162267880472457, 'recall': 0.9071682441386166, 'f1': 0.9071853154592542}}\n",
            "Iteration 8: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 2.65625 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9119, Precision: 0.9123, Recall: 0.9132, F1-Score: 0.9118\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9190, Precision: 0.9203, Recall: 0.9192, F1-Score: 0.9190\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.032233415933701\n",
            "{'dev': {'accuracy': 0.911875, 'precision': 0.9123334000150034, 'recall': 0.9132226745123959, 'f1': 0.9118498978810763}, 'test': {'accuracy': 0.919, 'precision': 0.9203391633005165, 'recall': 0.9192420034088571, 'f1': 0.9189616638020823}}\n",
            "Iteration 9: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 2.8125 % of data.\n",
            "Evaluating on development dataset:\n",
            "Development Set - Accuracy: 0.9194, Precision: 0.9227, Recall: 0.9174, F1-Score: 0.9188\n",
            "Evaluating on test dataset:\n",
            "Test Set - Accuracy: 0.9193, Precision: 0.9216, Recall: 0.9190, F1-Score: 0.9191\n",
            " =  =  =  =  =  =  =  =  =  =  =  = \n",
            "Average loss: 0.11418239533852198\n",
            "{'dev': {'accuracy': 0.919375, 'precision': 0.9227326950044099, 'recall': 0.9173809945197884, 'f1': 0.9187795206338975}, 'test': {'accuracy': 0.91925, 'precision': 0.9215707954791817, 'recall': 0.9189516086438634, 'f1': 0.9190951430472388}}\n",
            "Iteration 10: Model trained on newly labeled samples. Total rewards: -0.49999999999999994\n",
            "Using 2.96875 % of data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "for i in labeled_dataset:\n",
        "    cnt += 1"
      ],
      "metadata": {
        "id": "zXps_cRhrq2z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt"
      ],
      "metadata": {
        "id": "BQtVvUSWh0UM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe9ffc93-a8e5-4cbd-f0c1-02476ab216a3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ShVUv7f_ugN"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}